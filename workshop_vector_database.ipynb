{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvQJb30ZQo73nm0X+sm2JO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4850e256"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ogirimah/generative-ai-workshop/blob/main/workshop_vector_database.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ogirimah/generative-ai-workshop/workshop_vector_database.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq \\\n",
        "  openai \\\n",
        "  langchain \\\n",
        "  pinecone-client \\\n",
        "  tiktoken \\\n",
        "  datasets"
      ],
      "metadata": {
        "id": "MhLqL_ogQAPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfD76GAWbvsu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "480c58cd-f9bd-4607-9e19-ca53ec836bd4"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3780220c"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata\n",
        "# api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "from google.colab import userdata\n",
        "import getpass\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a6f7d30-0a2c-46d4-bd19-dae2772ab818"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "chat_client = ChatOpenAI(\n",
        "    openai_api_key = openai_api_key,\n",
        "    model_name = 'gpt-4',\n",
        "    temperature=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature: Determines the randomness of the models predictions. The higher the value, the more random and creative the model will be with its response. Mostly between 0 and 1"
      ],
      "metadata": {
        "id": "pIlmHSmzxL_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "model_name = 'text-embedding-ada-002' #\n",
        "\n",
        "embeder = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=openai_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "yMdgpjq9egQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " text-embedding-ada-002 is used by most OpenAI models, it is one of the cheapest and best performing"
      ],
      "metadata": {
        "id": "0iiXsTjngwFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_tesxt = [\n",
        "     'This is used by most OpenAI models',\n",
        "     'it is also one of the cheapest and best perfoming'\n",
        "]\n",
        "\n",
        "result = embeder.embed_documents(test_tesxt)\n",
        "len(result), len(result[0])"
      ],
      "metadata": {
        "id": "l1n6MOUF_SDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Database (Pinecone)\n",
        "\n",
        "We first create a pinecone account and then create an API key.\n",
        "\n",
        "We could also experiment with other vector databases that run on your local machine i.e. Lance, FAISS, Chroma and Qdrant. Details here: https://python.langchain.com/docs/modules/data_connection/vectorstores/"
      ],
      "metadata": {
        "id": "c2V-y7lnJRVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'llm-workshop-retrieval-augmentation'"
      ],
      "metadata": {
        "id": "3K47fsQCJvOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "\n",
        "PINECONE_API_KEY = userdata.get('pinecone-llmrag')\n",
        "\n",
        "# Environemt is next to API key in console\n",
        "PINECONE_ENVIRONMENT = userdata.get('Pinecone-Environment')\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    environment=PINECONE_ENVIRONMENT\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index if it does not exist\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='cosine',\n",
        "        dimension=len(result[0])  # 1536 dim of text-embedding-ada-002,\n",
        "        # We could also hard-code the dimension, but this is better\n",
        "    )"
      ],
      "metadata": {
        "id": "3byVZc14KAcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to the index and view its characteristics"
      ],
      "metadata": {
        "id": "r8aeHnIdOPYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_index = pinecone.Index(index_name)\n",
        "# Use pinecone.GRPCIndex, it has beter performance,\n",
        "# But you need to use pinecone-client[grpc] and not just pinecone-client\n",
        "\n",
        "pinecone_index.describe_index_stats()"
      ],
      "metadata": {
        "id": "4Q81Rj_KOXP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pinecone index should have no namespaces and vector_count of zero. This will be populated once we have added our vector. Note that if you are re-running thos scripts at a later time after adding data, it will not be zero"
      ],
      "metadata": {
        "id": "eptZjkn3QoCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset from huggingface hub\n",
        "\n",
        "We will use Huggingface Dataset library to load the dataset, and view the content of the first index"
      ],
      "metadata": {
        "id": "vpik0jXVQTlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ogirimah/ask_herts\")\n",
        "\n",
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "NbU6xi2tQuA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing\n",
        "\n"
      ],
      "metadata": {
        "id": "_YH5bI6TV2PY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Vector Store\n",
        "\n",
        "We will now use langchain to create a vector store using the pinecone index we created above"
      ],
      "metadata": {
        "id": "tHh2lO-WlpyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')"
      ],
      "metadata": {
        "id": "hbSJ6fZ3YOWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the token length function and test it\n",
        "def token_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "token_len('This is just a sample text to test the token_len function'\n",
        "          'The token length of this function is found below')"
      ],
      "metadata": {
        "id": "b_SilZ_WftEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
        "    length_function=token_len,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")"
      ],
      "metadata": {
        "id": "pRTPVJGiFf50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "index = pinecone.Index(index_name)\n",
        "vector_store = Pinecone(index, embeder, 'text')\n",
        "# vector_store = pinecone(index)"
      ],
      "metadata": {
        "id": "O9M4w_jrjtkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "\n",
        "batch_limit = 100\n",
        "\n",
        "texts = []\n",
        "metadatas = []\n",
        "documents = dataset['train']\n",
        "\n",
        "for i, record in enumerate(tqdm(documents)):\n",
        "    # first get metadata fields for this record\n",
        "    metadata = {\n",
        "        'doc-id': str(record['id']),\n",
        "        'source': record['source'],\n",
        "        # 'title': record['title'] # Use regular expression to take the string after the last /\n",
        "    }\n",
        "    # now we create chunks from the record text\n",
        "    record_texts = text_splitter.split_text(record['text'])\n",
        "    # create individual metadata dicts for each chunk\n",
        "    record_metadatas = [{\n",
        "        \"chunk\": j, \"text\": text, **metadata\n",
        "    } for j, text in enumerate(record_texts)]\n",
        "    # append these to current batches\n",
        "    texts.extend(record_texts)\n",
        "    metadatas.extend(record_metadatas)\n",
        "    # if we have reached the batch_limit we can add texts\n",
        "    if len(texts) >= batch_limit:\n",
        "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "        embeds = embeder.embed_documents(texts)\n",
        "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "        texts = []\n",
        "        metadatas = []\n",
        "\n",
        "if len(texts) > 0:\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    embeds = embeder.embed_documents(texts)\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas))"
      ],
      "metadata": {
        "id": "oUu_F-ceRSKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.describe_index(index_name)"
      ],
      "metadata": {
        "id": "BoF00S9PId0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What is the meaning of LRC'\n",
        "\n",
        "vector_store.similarity_search(query, k=1)\n",
        "\n",
        "# I need to go back and make sure the document has a text key"
      ],
      "metadata": {
        "id": "r1wJY5iMnNWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain. import H"
      ],
      "metadata": {
        "id": "sJhO7u__dAVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Models\n",
        "\n",
        "We are all mostly familiar with ChatGPT, but there are others\n",
        "\n",
        "**Bing Chat**\n",
        "  Integrated into microsoft edge browser, available as mobile app, and online. And recently it was released inPreview on windows 11\n",
        "\n",
        "**Claude**\n",
        "  Antropics LLM that is an alternative to ChatGPT. - https://claude.ai"
      ],
      "metadata": {
        "id": "4keB2aLJlI_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Platforms\n",
        "\n",
        "**Nvidia** **Nemo**\n",
        "  A toolkit for building conversational AI models. It is a part of the Nvidia AI platform. - https://www.nvidia.com/en-us/ai-data-science/products/nemo/get-started/?nvid=nv-int-unbr-268853\n",
        "\n",
        "**AWS** **SageMaker**\n",
        "\n",
        "  Amazon platform for building, training and deploying machine learning (ML) models - https://aws.amazon.com/sagemaker/\n",
        "\n",
        "**AWS** **Bedrock**\n",
        "\n",
        "  Amazon platform for working ith foundation generative models - https://aws.amazon.com/bedrock/\n",
        "\n",
        "**AWS** **Partyrock**\n",
        "\n",
        "  An Amazon Bedrock Playground - https://partyrock.aws/"
      ],
      "metadata": {
        "id": "GxpM23Z_eYdB"
      }
    }
  ]
}