{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4850e256",
      "metadata": {
        "id": "4850e256"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ogirimah/generative-ai-workshop/blob/main/workshop.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ogirimah/generative-ai-workshop/workshop.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies**\n",
        "\n",
        "\n",
        "*   OpenAI\n",
        "*   Langchain\n",
        "*   Tiktoken\n",
        "*   Ruby\n",
        "*   Wayback_machine_downloader\n",
        "*   Unstructured"
      ],
      "metadata": {
        "id": "0Jv5vjAMPKPJ"
      },
      "id": "0Jv5vjAMPKPJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MHlWnzZaYiT9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MHlWnzZaYiT9"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq \\\n",
        "  openai \\\n",
        "  langchain \\\n",
        "  tiktoken \\\n",
        "  unstructured"
      ],
      "metadata": {
        "id": "MhLqL_ogQAPB"
      },
      "id": "MhLqL_ogQAPB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install -q ruby-full\n",
        "!gem install -q wayback_machine_downloader"
      ],
      "metadata": {
        "id": "i-gk5hygQM_w"
      },
      "id": "i-gk5hygQM_w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wayback_machine_downloader https://ask.herts.ac.uk"
      ],
      "metadata": {
        "id": "i_VMuu_SUFpO"
      },
      "id": "i_VMuu_SUFpO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./drive/MyDrive/delete_script delete_script\n",
        "!chmod +x delete_script\n",
        "!./delete_script"
      ],
      "metadata": {
        "id": "7SC1oSShGfgU"
      },
      "id": "7SC1oSShGfgU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader('websites')\n",
        "docs = loader.load()\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "AQF273RkUFml"
      },
      "id": "AQF273RkUFml",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "1YNcz-sQUFhW"
      },
      "id": "1YNcz-sQUFhW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "UCsIBO4cUFev"
      },
      "id": "UCsIBO4cUFev",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[5].page_content)"
      ],
      "metadata": {
        "id": "5kK8tL55UFcH"
      },
      "id": "5kK8tL55UFcH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[6].metadata['source'].replace('websites/', 'https://')"
      ],
      "metadata": {
        "id": "9giby_BSYOZ4"
      },
      "id": "9giby_BSYOZ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tiktoken.encoding_name_for_model('gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "hbSJ6fZ3YOWs"
      },
      "id": "hbSJ6fZ3YOWs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('cl100k_base')"
      ],
      "metadata": {
        "id": "-QVe6mt8fzDx"
      },
      "id": "-QVe6mt8fzDx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the token length function and test it\n",
        "def token_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "token_len('This is just a sample text to test the token_len function'\n",
        "          'The token length of this function is found below')"
      ],
      "metadata": {
        "id": "b_SilZ_WftEr"
      },
      "id": "b_SilZ_WftEr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_counts = [token_len(doc.page_content) for doc in docs]"
      ],
      "metadata": {
        "id": "wCMgMaVBYOMz"
      },
      "id": "wCMgMaVBYOMz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"Min: {min(token_counts)}\n",
        "Avg: {int(sum(token_counts) / len(token_counts))}\n",
        "Max: {max(token_counts)}\"\"\")"
      ],
      "metadata": {
        "id": "bI1pdRWOYOKF"
      },
      "id": "bI1pdRWOYOKF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
        "    length_function=token_len,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")"
      ],
      "metadata": {
        "id": "I_Ocr88HUFZH"
      },
      "id": "I_Ocr88HUFZH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = text_splitter.split_text(docs[6].page_content)\n",
        "len(chunks)"
      ],
      "metadata": {
        "id": "ke4FrwWWY4-N"
      },
      "id": "ke4FrwWWY4-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "hasher = hashlib.md5()  # this will convert URL into unique ID\n",
        "\n",
        "url = docs[5].metadata['source'].replace('websites/', 'https://')\n",
        "print(url)\n",
        "\n",
        "# convert URL to unique ID\n",
        "hasher.update(url.encode('utf-8'))\n",
        "unique_id = hasher.hexdigest()[:12]\n",
        "print(unique_id)"
      ],
      "metadata": {
        "id": "rI0ghZWiY47K"
      },
      "id": "rI0ghZWiY47K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    {\n",
        "        'id': f'{unique_id}-{i}',\n",
        "        'text': text_chunk,\n",
        "        'source': url\n",
        "    } for i, text_chunk in enumerate(chunks)\n",
        "]\n",
        "data"
      ],
      "metadata": {
        "id": "7cfT17KfY44W"
      },
      "id": "7cfT17KfY44W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "documents = []\n",
        "\n",
        "for doc in tqdm(docs):\n",
        "    url = doc.metadata['source'].replace('websites/', 'https://')\n",
        "    hasher.update(url.encode('utf-8'))\n",
        "    unique_id = hasher.hexdigest()[:12]\n",
        "    chunks = text_splitter.split_text(doc.page_content)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        documents.append({\n",
        "            'id': f'{unique_id}-{i}',\n",
        "            'text': chunk,\n",
        "            'source': url\n",
        "        })\n",
        "\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "Ck2N_ZaYY41n"
      },
      "id": "Ck2N_ZaYY41n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save the .json1 file"
      ],
      "metadata": {
        "id": "NAoh4Ce4vkY2"
      },
      "id": "NAoh4Ce4vkY2"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('ask_herts.jsonl', 'w') as f:\n",
        "    for doc in documents:\n",
        "        f.write(json.dumps(doc) + '\\n')"
      ],
      "metadata": {
        "id": "IGsZVEekY4y1"
      },
      "id": "IGsZVEekY4y1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}